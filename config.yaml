
# settings specific to camera handling
camera:
  # camera id for cv::VideoCapture constructor (probably '0' if only one camera is present)
  camId: 0

# settings for audio capture/playback
audio:

  aiActivationWords:
    - "chat"

  cmdActivationWords:
    - "camera"
    - "alex"  #don't change this one - it is the safe "always" working word

  #set either to false to perform online speech recognition before final processing
  cmdLocalSpeechDetectOnly: true
  aiLocalSpeechDetectOnly: true


  #microphone device to use
  device: "default"

  echoSpeech: false
  readTextBack: false


  #microphone samples per second
  samplesPerSec: 16000
  #how many samples between each check for speech
  #also means minimum silence required to detect end of speech
  samplesPerCheck: 12000

whisper:
  vadModel: "./models/ggml-silero-v5.1.2.bin"
  vadThreshold: 0.95
  model: "./models/ggml-base.en.bin"
  maxThreads: 6

espeak:
  data: "./build/espeak/src/espeakng-build/espeak-ng-data"
  model: "./models/en_GB-cori-medium.onnx"

#settings for openai integration
openai:
  #full path to file containing the openai api key
  keyFile: "./oai_key"

  #for different types of requests different openai models can be used a various speed and price options
  #Details of models available can be found here: https://platform.openai.com/docs/models

  #ai model to use for text to audio api requests (must support audio output)
  model: "gpt-4o-mini-audio-preview"
  #ai model to use for speech to text requests (must support transcribe operation)
  transcribeModel: "gpt-4o-mini-transcribe"
  #ai model to use for image analysing (will not also support audio output currently)
  imageModel: "gpt-4.1-mini"
  #ai moddel to use for text to speech requests
  ttsModel: "gpt-4o-mini-tts"

  #openai voice to use for speech output (openai supports several voices: https://platform.openai.com/docs/guides/text-to-speech)
  voice: "nova"

  #if these words appear in the AI request text, the current image frame will be included
  imageInclusionKeywords:
    - "image" #e.g. describe this image
    - "seeing" #e.g. describe what I am seeing
    - "photo" #e.g. what is in this photo?
    - "looking" #e.g. what am i looking at?
    - "picture" #e.g. what is in this picture?
    - "read" #e.g. read this 

#settings for vision

Imaging:
  # the initial settings for the imaging modes
  imagemode: 1
  imagezoom: 0
  imageedgeno: 50
  imagethresh-lev: 128
  imagethreshmode: 0
  
  usedebugcamera: false
  glassesfullscreen: false

RoboRob:

  # these words are to trigger the commands
  # there must be a space before and after to make sure we don't match a partial word
  zoomin: " in "
  zoomout: " out "
  edges: " edges "
  normal: " normal "
  contrast: " contrast "
  more: " more "
  less: " less "
  flip: " flip "
  help: " help "
  note: " notes "
  noteadd: " make "
  noteclear: " delete "
  noteread: " read "
  mute: " mute on "
  unmute: " mute off "
  main: " main " # to get the generic help rather than the mode specific help

   # way to set the audio i/o up. no spaces needed here 
  speakervolume: "100%"
  speakervolmessage: "pactl set-sink-volume alsa_output.usb-Vendor_Air_A00011_32_00-00.analog-stereo "
  defaultmic: "pactl set-default-source alsa_input.usb-Vendor_Air_A00011_32_00-00.mono-fallback"
  defaultspeaker: "pactl set-default-sink alsa_output.usb-Vendor_Air_A00011_32_00-00.analog-stereo"
  micvol: "pactl set-source-volume alsa_input.usb-Vendor_Air_A00011_32_00-00.mono-fallback 100%"


#options to control log output
logging:
  logToConsole: true
  logToFile: true
  logFile: "./glasses.log"
